{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "安裝必要套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 --index-url https://download.pytorch.org/whl/cu118\n",
    "! pip install immutabledict==4.1.0 sentencepiece==0.1.99 \n",
    "! pip install kagglehub\n",
    "! mkdir model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "引入必要套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, torch, threading\n",
    "import tkinter as tk\n",
    "sys.path.append('gemma_pytorch')\n",
    "from gemma_pytorch.gemma.config import get_config_for_2b\n",
    "from gemma_pytorch.gemma.model import GemmaForCausalLM"
   ]
  },
  {
   "attachments": {
    "image-2.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "檢查是否已將模型放入 'model' 資料夾\n",
    "\n",
    "1. 到此連結下載: https://www.kaggle.com/models/google/gemma/pyTorch/1.1-2b-it\n",
    "\n",
    "2. 選擇 2b-it 結尾的版本，前面數字越大版本越新 (需要登入 Kaggle 帳號才能下載)\n",
    "\n",
    "3. 將檔案解壓縮並貼到 'model' 資料夾內\n",
    "\n",
    "![image-2.png](attachment:image-2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that the tokenizer is present\n",
    "tokenizer_path = os.path.join('model', 'tokenizer.model')\n",
    "assert os.path.isfile(tokenizer_path), 'Tokenizer not found!'\n",
    "# Ensure that the checkpoint is present\n",
    "ckpt_path = os.path.join('model', 'gemma-2b-it.ckpt')\n",
    "assert os.path.isfile(ckpt_path), 'PyTorch checkpoint not found!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定義 PyTorch 的運算設備與使用模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.2.1+cu118 -- using cuda\n"
     ]
    }
   ],
   "source": [
    "def define_device():\n",
    "    torch_version = torch.__version__\n",
    "    print(f\"PyTorch version: {torch_version}\", end=\" -- \")\n",
    "\n",
    "    # Check if MPS (Multi-Process Service) device is available on MacOS\n",
    "    if torch.backends.mps.is_available():\n",
    "        # If MPS is available, print a message indicating its usage\n",
    "        print(\"using MPS device on MacOS\")\n",
    "        # Define the device as MPS\n",
    "        defined_device = torch.device(\"mps\")\n",
    "    else:\n",
    "        # If MPS is not available, determine the device based on GPU availability\n",
    "        defined_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # Print a message indicating the selected device\n",
    "        print(f\"using {defined_device}\")\n",
    "\n",
    "    return defined_device\n",
    "\n",
    "def define_model():\n",
    "    model_config = get_config_for_2b()\n",
    "    model_config.tokenizer = tokenizer_path\n",
    "    model_config.quant = False\n",
    "    torch.set_default_dtype(model_config.get_dtype())\n",
    "    define_model = GemmaForCausalLM(model_config)\n",
    "    define_model.load_weights(ckpt_path)\n",
    "    define_model = define_model.to(device).eval()\n",
    "    return define_model\n",
    "\n",
    "device = define_device()\n",
    "model = define_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定義對話模型的基本結構和功能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GemmaAI:\n",
    "    def __init__(self, model, device):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.output_len = 20\n",
    "        self.RECORD = []\n",
    "        self.USER_CHAT_TEMPLATE = '<start_of_turn>user\\n{prompt}<end_of_turn>\\n'\n",
    "        self.MODEL_CHAT_TEMPLATE = '<start_of_turn>model\\n{prompt}<end_of_turn>\\n'\n",
    "\n",
    "    def generatePrompt(self):\n",
    "        prompt = ''\n",
    "        for i in range(len(self.RECORD)):\n",
    "            prompt += self.USER_CHAT_TEMPLATE.format(prompt=self.RECORD[i]) if i % 2 == 0 else self.MODEL_CHAT_TEMPLATE.format(prompt=self.RECORD[i])\n",
    "        prompt += '<start_of_turn>model\\n'\n",
    "        return prompt\n",
    "\n",
    "    def generateReply(self):\n",
    "        return self.model.generate(self.generatePrompt(), device=self.device, output_len=self.output_len)\n",
    "\n",
    "    def generateTranslate(self, prompt, output_len):\n",
    "        prompt = self.USER_CHAT_TEMPLATE.format(prompt=prompt) + '<start_of_turn>model\\n'\n",
    "        return self.model.generate(prompt, device=self.device, output_len=output_len)\n",
    "\n",
    "    def user_last_text(self):\n",
    "        return '' if len(self.RECORD) == 0 else self.RECORD[-2 + len(self.RECORD) % 2]\n",
    "\n",
    "    def model_last_text(self):\n",
    "        return '' if len(self.RECORD) < 1 or len(self.RECORD) % 2 != 0 else self.RECORD[-1]\n",
    "\n",
    "    def updateOutputLen(self, val):\n",
    "        self.output_len = int(val)\n",
    "\n",
    "AI = GemmaAI(model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "建立 Tkinter 視窗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_main_widget():\n",
    "    def disabled_button():\n",
    "        infoBtn['state'] = 'disabled'\n",
    "        sendBtn['state'] = 'disabled'\n",
    "        aiBtn['state'] = 'disabled'\n",
    "        chatBtn['state'] = 'disabled'\n",
    "    def normal_button():\n",
    "        infoBtn['state'] = 'normal'\n",
    "        sendBtn['state'] = 'normal'\n",
    "        aiBtn['state'] = 'normal'\n",
    "        chatBtn['state'] = 'normal' if len(AI.RECORD) > 1 else 'disabled'\n",
    "    def sendBtn_thread():\n",
    "        reply = AI.generateReply()\n",
    "        AI.RECORD.append(reply)\n",
    "        text.delete(1.0, tk.END)\n",
    "        text.insert(tk.END, reply)\n",
    "        normal_button()\n",
    "    def sendBtn_click():\n",
    "        disabled_button()\n",
    "        msg = entry.get()\n",
    "        AI.RECORD = [msg]\n",
    "        text.delete(1.0, tk.END)\n",
    "        text.insert(tk.END, '~正在努力思考中~')\n",
    "        thread = threading.Thread(target=sendBtn_thread)\n",
    "        thread.start()\n",
    "    def aiBtn_thread(prompt, output_len):\n",
    "        translation = AI.generateTranslate(prompt, output_len)\n",
    "        text.delete(1.0, tk.END)\n",
    "        text.insert(tk.END, translation)\n",
    "        normal_button()\n",
    "    def aiBtn_click():\n",
    "        disabled_button()\n",
    "        prompt = '翻譯成繁中\\n{}'.format(text.get(1.0, tk.END))\n",
    "        output_len = len(prompt)*2\n",
    "        text.delete(1.0, tk.END)\n",
    "        if output_len > 0:\n",
    "            text.insert(tk.END, '~正在努力翻譯中~')\n",
    "            thread = threading.Thread(target=aiBtn_thread, args=(prompt, output_len))\n",
    "            thread.start()\n",
    "        else:\n",
    "            text.insert(tk.END, '請在此處輸入需翻譯之內容')\n",
    "    def chatBtn_thread():\n",
    "        reply = AI.generateReply()\n",
    "        AI.RECORD.append(reply)\n",
    "        text.delete(1.0, tk.END)\n",
    "        text.insert(tk.END, reply)\n",
    "        normal_button()\n",
    "    def chatBtn_click():\n",
    "        disabled_button()\n",
    "        msg = entry.get()\n",
    "        AI.RECORD.append(msg)\n",
    "        text.delete(1.0, tk.END)\n",
    "        text.insert(tk.END, '~正在努力思考中~')\n",
    "        thread = threading.Thread(target=chatBtn_thread)\n",
    "        thread.start()\n",
    "    \n",
    "    for widget in root.winfo_children():\n",
    "        widget.destroy()\n",
    "    frame = tk.Frame(root)\n",
    "    frame.grid(row=0, column=0, sticky='nsew')\n",
    "    frame.grid_rowconfigure(1, weight=1)\n",
    "    frame.grid_columnconfigure(0, weight=1)\n",
    "\n",
    "    entryButtons = tk.Frame(frame)\n",
    "    entryButtons.grid(row=0, column=0, sticky='ew')\n",
    "    infoBtn = tk.Button(entryButtons, bitmap='info', command=lambda: create_info_widget())\n",
    "    infoBtn.pack(side=tk.LEFT)\n",
    "    entry = tk.Entry(entryButtons)\n",
    "    entry.pack(side=tk.LEFT, fill='x', expand=1)\n",
    "    sendBtn = tk.Button(entryButtons, text=\"送出\", command=lambda: sendBtn_click())\n",
    "    sendBtn.pack(side=tk.LEFT)\n",
    "\n",
    "    text = tk.Text(frame)\n",
    "    text.grid(row=1, column=0, sticky='nsew')\n",
    "\n",
    "    buttons = tk.Frame(frame)\n",
    "    buttons.grid(row=2, column=0, sticky='ew')\n",
    "    aiBtn = tk.Button(buttons, text=\"智能翻譯\", command=lambda: aiBtn_click())\n",
    "    aiBtn.pack(side=tk.LEFT, fill='x', expand=1)\n",
    "    chatBtn = tk.Button(buttons, text=\"繼續對話\", command=lambda: chatBtn_click())\n",
    "    chatBtn.pack(side=tk.LEFT, fill='x', expand=1)\n",
    "    entry.insert(tk.END, AI.user_last_text())\n",
    "    text.insert(tk.END, AI.model_last_text())\n",
    "    normal_button()\n",
    "\n",
    "def create_info_widget():\n",
    "    for widget in root.winfo_children():\n",
    "        widget.destroy()\n",
    "    frame = tk.Frame(root)\n",
    "    frame.pack()\n",
    "\n",
    "    label1 = tk.Label(frame, text='使用 {} 進行運算\\n'.format(str(device)))\n",
    "    label1.pack()\n",
    "\n",
    "    labels = tk.Frame(frame)\n",
    "    labels.pack()\n",
    "    label2 = tk.Label(labels, text='對話長度上限:')\n",
    "    label2.pack(side=tk.LEFT)\n",
    "    label3 = tk.Label(labels, textvariable=tk_output_len)\n",
    "    label3.pack(side=tk.LEFT)\n",
    "\n",
    "    scale_h = tk.Scale(frame, from_=20, to=2000, orient='horizontal', showvalue=False, variable=tk_output_len, command=AI.updateOutputLen)\n",
    "    scale_h.pack()\n",
    "\n",
    "    label4 = tk.Label(frame, text='上限越高輸出時間越長\\n', font=('Arial',8), fg='#f00')\n",
    "    label4.pack()\n",
    "\n",
    "    backBtn = tk.Button(frame, text=\"返回\", command=lambda: create_main_widget())\n",
    "    backBtn.pack()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 創建主視窗\n",
    "    root = tk.Tk()\n",
    "    root.title('FloatChat')\n",
    "    root.minsize(200, 160)\n",
    "    root.attributes('-alpha', 0.8)\n",
    "\n",
    "    # 獲取螢幕的寬度和高度\n",
    "    screen_width = root.winfo_screenwidth()\n",
    "    screen_height = root.winfo_screenheight()\n",
    "\n",
    "    # 設定視窗的大小和位置\n",
    "    root.geometry('200x160+{}+0'.format(screen_width - 200))\n",
    "\n",
    "    # 讓視窗保持在最上層\n",
    "    root.attributes('-topmost', True)\n",
    "\n",
    "    root.grid_rowconfigure(0, weight=1)\n",
    "    root.grid_columnconfigure(0, weight=1)\n",
    "\n",
    "    tk_output_len = tk.IntVar()\n",
    "    tk_output_len.set(AI.output_len)\n",
    "    create_main_widget()\n",
    "\n",
    "    # 開始主循環\n",
    "    root.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
